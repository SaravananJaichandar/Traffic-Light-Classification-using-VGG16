{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning for Image Classification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Using the Transfer Learning technique, we are classifiying the traffic light signals. So inorder to classify the traffic light signals, we are importing the necessary libraries and layers for building the neural network. Here, Keras is used for Transfer Learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\programfiles\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense,Conv2D,MaxPooling2D,Dropout,Flatten,Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# In this classification, we are going to classify three classes of traffic light signals - Red, Green and Yellow. The shape of the input images is expected to be 224,224 with RGB color channel at the last. Also, the pre-trained model VGG16 is used for classifying the traffic lights. \n",
    "\n",
    "# VGG16 - Visual Geometry Group was a renowned oxford group, who developed and trained this model with Imagenet database. So we are importing the model with the top layers removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes=3\n",
    "img_size=224\n",
    "image_input = Input(shape=(224, 224, 3))\n",
    "model = VGG16(input_tensor=image_input, include_top=False, weights= 'imagenet')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# We are building our own neural network to add to the top of the VGG16 for predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = Sequential()\n",
    "top_model.add(model)\n",
    "top_model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=model.output_shape[1:],padding='same'))\n",
    "top_model.add(MaxPooling2D((2,2), padding='same'))\n",
    "top_model.add(Dropout(0.25))\n",
    "top_model.add(Conv2D(64, kernel_size= (3,3),activation='relu',padding='same'))\n",
    "top_model.add(MaxPooling2D((2,2), padding='same'))\n",
    "top_model.add(Flatten())\n",
    "top_model.add(Dense(128,activation='relu'))\n",
    "top_model.add(Dense(num_classes,activation='softmax'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Model summary of the model which we have built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 32)          147488    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 14,913,955\n",
      "Trainable params: 14,913,955\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "top_model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Freezing the VGG16 model, because it is already trained with Imagenet database, so we dont need to train this model and train only our top model which we have built, so that the VGG16 model extracts the patterns and feature vectors, which will be used by the top layers for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 32)          147488    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 14,913,955\n",
      "Trainable params: 199,267\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layer in top_model.layers[:-8]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "    \n",
    "top_model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Specifying the optimizers, loss and evalutations metrics for our final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model.compile(optimizer = 'Adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Its time to fit our hybrid model with the train and test data which we have built so far. As you can see, Early stopping metric is used to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 450 images belonging to 3 classes.\n",
      "Found 60 images belonging to 3 classes.\n",
      "Epoch 1/50\n",
      "10/10 [==============================] - 98s 10s/step - loss: 1.1700 - acc: 0.3549 - val_loss: 1.0923 - val_acc: 0.3833\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 101s 10s/step - loss: 1.0450 - acc: 0.4688 - val_loss: 1.0110 - val_acc: 0.6167\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 93s 9s/step - loss: 0.9106 - acc: 0.5662 - val_loss: 0.8076 - val_acc: 0.7000\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 95s 9s/step - loss: 0.7721 - acc: 0.6056 - val_loss: 0.6728 - val_acc: 0.7500\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 94s 9s/step - loss: 0.6390 - acc: 0.7729 - val_loss: 0.6067 - val_acc: 0.6000\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 100s 10s/step - loss: 0.4944 - acc: 0.7875 - val_loss: 0.4266 - val_acc: 0.8167\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 95s 10s/step - loss: 0.3962 - acc: 0.8091 - val_loss: 0.3276 - val_acc: 0.8500\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 96s 10s/step - loss: 0.3128 - acc: 0.8738 - val_loss: 0.1581 - val_acc: 0.9667\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 111s 11s/step - loss: 0.2340 - acc: 0.9156 - val_loss: 0.2647 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 99s 10s/step - loss: 0.2561 - acc: 0.8817 - val_loss: 0.3471 - val_acc: 0.8500\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 108s 11s/step - loss: 0.2768 - acc: 0.8844 - val_loss: 0.2342 - val_acc: 0.8667\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=3,\n",
    "                          verbose=1, mode='auto')\n",
    "callbacks_list = [earlystop]\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "data_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "data_generator_with_aug = ImageDataGenerator(horizontal_flip=True,\n",
    "                                            rescale=1./255,\n",
    "                                            width_shift_range=0.2,\n",
    "                                            height_shift_range=0.2)\n",
    "\n",
    "train_generator = data_generator_with_aug.flow_from_directory(r'E:\\Thesis\\Try1\\Train',\n",
    "                                                             batch_size=batch_size,\n",
    "                                                             target_size=(img_size,img_size),\n",
    "                                                             class_mode='categorical')\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(r'E:\\Thesis\\Try1\\Test',\n",
    "                                                         batch_size=batch_size,\n",
    "                                                         target_size=(img_size,img_size),\n",
    "                                                         class_mode='categorical') \n",
    "history = top_model.fit_generator(train_generator,\n",
    "                   steps_per_epoch=10,\n",
    "                   epochs=50,\n",
    "                   validation_data = validation_generator,\n",
    "                   validation_steps = 1,\n",
    "                   callbacks=callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
